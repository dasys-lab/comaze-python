{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dasys-lab/comaze-python/blob/gym-env/CoMazeGym_Agent_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please, run the following cell alone, and then go directly to the subsection that you are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPALy2GEL_vQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "import gym\n",
    "\n",
    "\n",
    "class CoMazeGym(gym.Env):\n",
    "  if os.path.isfile(\".local\"):\n",
    "    API_URL = \"http://localhost:16216\"\n",
    "    WEBAPP_URL = \"http://localhost\"\n",
    "  else:\n",
    "    API_URL = \"http://teamwork.vs.uni-kassel.de:16216\"\n",
    "    WEBAPP_URL = \"http://teamwork.vs.uni-kassel.de\"\n",
    "  LIB_VERSION = \"1.3.0\"\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.game = None\n",
    "    self.game_id = None\n",
    "    self.player_id = None\n",
    "    self.action_space = None\n",
    "\n",
    "  def reset(self, options={}):\n",
    "    level = options.get(\"level\", \"1\")\n",
    "    num_of_player_slots = options.get(\"num_of_player_slots\", \"2\")\n",
    "    \n",
    "    self.game_id = options.get(\"game_id\", None)\n",
    "    if self.game_id is None:\n",
    "      self.game_id = requests.post(self.API_URL + \"/game/create?level=\" + level + \"&numOfPlayerSlots=\" + num_of_player_slots).json()[\"uuid\"]\n",
    "      options[\"game_id\"] = self.game_id\n",
    "    \n",
    "    return self.play_existing_game(options)\n",
    "\n",
    "  def play_existing_game(self, options={}):\n",
    "    if \"look_for_player_name\" in options:\n",
    "      options[\"game_id\"] = requests.get(self.API_URL + \"/game/byPlayerName?playerName=\" + options[\"look_for_player_name\"]).json()[\"uuid\"]\n",
    "\n",
    "    if \"game_id\" not in options or len(options[\"game_id\"]) != 36:\n",
    "      raise Exception(\"You must provide a game id when attending an existing game. Use play_new_game() instead of play_existing_game() if you want to create a new game.\")\n",
    "\n",
    "    player_name = options.get(\"player_name\", \"Python\")\n",
    "    self.game_id = options[\"game_id\"]\n",
    "    print(\"Joined gameId: \" + self.game_id)\n",
    "    player = requests.post(self.API_URL + \"/game/\" + self.game_id + \"/attend?playerName=\" + player_name).json()\n",
    "    self.player_id = player[\"uuid\"]\n",
    "    self.action_space = player['directions'] + ['SKIP']\n",
    "    print(\"Playing as playerId: \" + self.player_id)\n",
    "    self.game = requests.get(self.API_URL + \"/game/\" + self.game_id).json()\n",
    "    print(f'Action Space is {self.action_space}')\n",
    "\n",
    "    while self.game['currentPlayer']['uuid'] != self.player_id:\n",
    "      print(f'Waiting for other player to make first move')\n",
    "      print(\"(Invite someone: \" + self.WEBAPP_URL + \"/?gameId=\" + self.game_id + \")\")\n",
    "      time.sleep(1)\n",
    "      self.game = requests.get(self.API_URL + \"/game/\" + self.game_id).json()\n",
    "\n",
    "    return self.game\n",
    "\n",
    "  def step(self, action, message=None):\n",
    "    moved = False\n",
    "    while not moved:\n",
    "      self.game = requests.get(self.API_URL + \"/game/\" + self.game_id).json()\n",
    "\n",
    "      if not self.game[\"state\"][\"started\"]:\n",
    "        print(\"Waiting for players. (Invite someone: \" + self.WEBAPP_URL + \"/?gameId=\" + self.game_id + \")\")\n",
    "        time.sleep(3)\n",
    "        continue\n",
    "      available_actions = self.game[\"currentPlayer\"][\"directions\"]\n",
    "      if action not in available_actions:\n",
    "        print(f\"WARNING: Action {action} is not available to the current player.\")\n",
    "        action = \"SKIP\"\n",
    "      print(\"Moving \" + action)\n",
    "      if action == 'SKIP':\n",
    "        print(f'Wanted to send message {message}, but skipped.')\n",
    "        message = None\n",
    "      else:\n",
    "        print(f'Sending message {message}.')\n",
    "      print('---')\n",
    "      request_url = self.API_URL + \"/game/\" + self.game_id + \"/move\"\n",
    "      request_url += \"?playerId=\" + self.player_id\n",
    "      request_url += \"&action=\" + action\n",
    "      if message is not None and action != 'SKIP':\n",
    "        request_url += \"&symbolMessage=\" + message\n",
    "      print(request_url)\n",
    "      self.game = requests.post(request_url).json()\n",
    "      moved = True\n",
    "    \n",
    "    if self.game[\"state\"][\"won\"]:\n",
    "      print(\"Game won!\")\n",
    "      reward = 1\n",
    "    elif self.game[\"state\"][\"lost\"]:\n",
    "      print(\"Game lost (\" + self.game[\"state\"][\"lostMessage\"] + \").\")\n",
    "      reward = -1\n",
    "    else:\n",
    "      reward = 0\n",
    "\n",
    "    if not self.game[\"state\"][\"over\"]:\n",
    "      # wait for other player to make a move before sending back obs\n",
    "      while self.game['currentPlayer']['uuid'] != self.player_id:\n",
    "        print(f'Waiting for other player to make a move')\n",
    "        time.sleep(1)\n",
    "        self.game = requests.get(self.API_URL + \"/game/\" + self.game_id).json()\n",
    "\n",
    "    return self.game, reward, self.game[\"state\"][\"over\"], None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Agent and Gym-like env:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8fb8gjDM5jP"
   },
   "outputs": [],
   "source": [
    "env = CoMazeGym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5y_ulJCCM-Bi"
   },
   "outputs": [],
   "source": [
    "# Random Agent\n",
    "import random \n",
    "\n",
    "obs = env.reset()\n",
    "game_over = False\n",
    "while not game_over:\n",
    "  obs, reward, game_over, info = env.step(random.choice(env.action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ua-MVU_4PXo3"
   },
   "outputs": [],
   "source": [
    "# Nearest Goal Agent\n",
    "# Choose a nearest goal, see if one of your actions can get you there, if so take that action\n",
    "obs = env.reset()\n",
    "game_over = False\n",
    "action_space = env.action_space\n",
    "goals_pos = [goal['position']\n",
    "             for goal in obs['config']['goals']]\n",
    "\n",
    "while not game_over:\n",
    "  goals_pos = [goal['position'] for goal in obs['unreachedGoals']]\n",
    "  agent_pos = obs['agentPosition']\n",
    "  \n",
    "  goal_diffs = [(goal['x'] - agent_pos['x'], goal['y'] - agent_pos['y'])\n",
    "                for goal in goals_pos]\n",
    "  goal_dists = [abs(diff[0])+abs(diff[1]) for diff in goal_diffs]\n",
    "  nearest_goal = goal_dists.index(min(goal_dists)) \n",
    "\n",
    "  print(f'Nearest goal is {obs[\"unreachedGoals\"][nearest_goal]}')\n",
    "  print(f'Nearest goal diff {goal_diffs[nearest_goal]}')\n",
    "\n",
    "  move_x, move_y = goal_diffs[nearest_goal]\n",
    "\n",
    "  if 'LEFT' in action_space and move_x < 0:\n",
    "    action = 'LEFT'\n",
    "  elif 'RIGHT' in action_space and move_x > 0:\n",
    "    action = 'RIGHT'\n",
    "  elif 'UP' in action_space and move_y < 0:\n",
    "    action = 'UP'\n",
    "  elif 'DOWN' in action_space and move_y > 0:\n",
    "    action = 'DOWN'\n",
    "  else:\n",
    "    action = 'SKIP'\n",
    "\n",
    "  obs, reward, game_over, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CoMazeGym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic RL agent\n",
    "# single-layer NN that takes in partial observation of the current state (no walls)\n",
    "# and learns actions WITHOUT communication.\n",
    "\n",
    "ACTION_SPACE = ['LEFT', 'RIGHT', 'UP', 'DOWN', 'SKIP']\n",
    "\n",
    "class RLAgent(nn.Module):\n",
    "  def __init__(self, arena_size, num_actions=5):\n",
    "    super().__init__()\n",
    "    arena_size_flat = arena_size[0] * arena_size[1]\n",
    "    self.embed_state = nn.Linear(arena_size_flat,16)\n",
    "    self.embed_action_space = nn.Linear(5,5)\n",
    "    self.policy = nn.Linear(21,num_actions)\n",
    "\n",
    "  def forward(self, state, action_space):\n",
    "    state_emb = self.embed_state(state)\n",
    "    action_emb = self.embed_action_space(action_space)\n",
    "    state_action_emb = torch.cat((state_emb, action_emb), dim=1)\n",
    "    return self.policy(state_action_emb)\n",
    "\n",
    "\n",
    "def get_state_tensor(obs):\n",
    "  arena_size = (obs['config']['arenaSize']['x'], obs['config']['arenaSize']['y'])\n",
    "  state_tensor = torch.zeros(arena_size).float()\n",
    "  state_tensor[obs['agentPosition']['x']][obs['agentPosition']['y']] = 1    # agent\n",
    "\n",
    "  for goal in obs['unreachedGoals']:\n",
    "    state_tensor[goal['position']['x']][goal['position']['y']] = 2\n",
    "  \n",
    "  return state_tensor\n",
    "\n",
    "\n",
    "def calculate_returns(rewards, discount_factor, normalize = True):\n",
    "    returns = []\n",
    "    R = 0\n",
    "    \n",
    "    for r in reversed(rewards):\n",
    "        R = r + R * discount_factor\n",
    "        returns.insert(0, R)\n",
    "        \n",
    "    returns = torch.tensor(returns)\n",
    "    \n",
    "    if normalize:\n",
    "        returns = (returns - returns.mean()) / returns.std()\n",
    "\n",
    "    return returns\n",
    "\n",
    "\n",
    "action_space_list = [1 if x in env.action_space else 0 for x in ACTION_SPACE]\n",
    "action_space_tensor = torch.FloatTensor(action_space_list)\n",
    "action_space_tensor_batch = action_space_tensor.unsqueeze(0)\n",
    "\n",
    "discount_factor = 0.9\n",
    "learning_rate = 1e-2\n",
    "num_episodes = 1\n",
    "\n",
    "# arena_size = (obs['arenaSize']['x'], obs['arenaSize']['y'])\n",
    "arena_size = (7,7)\n",
    "agent = RLAgent(arena_size)\n",
    "optimizer = torch.optim.SGD(agent.parameters(), lr=learning_rate)\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "  obs = env.reset()\n",
    "\n",
    "  done = False\n",
    "  log_prob_actions = []\n",
    "  rewards = []\n",
    "  episode_reward = 0\n",
    "\n",
    "  while not done:\n",
    "    state_tensor = get_state_tensor(obs)\n",
    "    state_tensor_batch = torch.flatten(state_tensor).unsqueeze(0)\n",
    "    action_pred = agent(state_tensor_batch, action_space_tensor_batch)\n",
    "    \n",
    "    action_prob = F.softmax(action_pred, dim = -1)  \n",
    "    avail_action_prob = action_prob * action_space_tensor\n",
    "    dist = distributions.Categorical(avail_action_prob)\n",
    "    action = dist.sample()\n",
    "    log_prob_action = dist.log_prob(action)\n",
    "\n",
    "    obs, reward, done, _ = env.step(ACTION_SPACE[action.item()])\n",
    "\n",
    "    log_prob_actions.append(log_prob_action)\n",
    "    rewards.append(reward)\n",
    "\n",
    "    episode_reward += reward\n",
    "\n",
    "\n",
    "  log_prob_actions = torch.cat(log_prob_actions)\n",
    "  returns = calculate_returns(rewards, discount_factor).detach()\n",
    "  loss = - (returns * log_prob_actions).sum()\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  print(f'Loss {loss} EP reward {episode_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Action Space that combines direction/skip and message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Box, Discrete, Dict, MultiBinary\n",
    "import numpy as np\n",
    "\n",
    "class CoMazeGymActionWrapper(gym.Wrapper):\n",
    "  def __init__(self, env, vocab_size=10, maximum_sentence_length=1, options={}):\n",
    "    super(CoMazeGymActionWrapper, self).__init__(env)\n",
    "    self.nb_directions = 4\n",
    "    self.actionId2action =  [\"LEFT\", \"RIGHT\", \"UP\", \"DOWN\"]\n",
    "    self.action2actionId =  {\"LEFT\":0, \"RIGHT\":1, \"UP\":2, \"DOWN\":3}\n",
    "    \n",
    "    self.vocab_size = vocab_size\n",
    "    self.id2token = {\n",
    "      0:\"empty\", \n",
    "      1:\"Q\", \n",
    "      2:\"W\", \n",
    "      3:\"E\", \n",
    "      4:\"R\", \n",
    "      5:\"T\", \n",
    "      6:\"Y\", \n",
    "      7:\"U\", \n",
    "      8:\"I\", \n",
    "      9:\"O\", \n",
    "      10:\"P\"\n",
    "    }\n",
    "    self.maximum_sentence_length = maximum_sentence_length\n",
    "    self._build_sentenceId2sentence()\n",
    "    \n",
    "    self.nb_possible_actions = self.nb_directions*self.nb_possible_sentences+1 \n",
    "    # +1 accounts for the SKIP action...\n",
    "    self.action_space = Discrete(self.nb_possible_actions)\n",
    "\n",
    "  def _build_sentenceId2sentence(self):\n",
    "    self.nb_possible_sentences = 1 # account for the empty string:\n",
    "    for pos in range(self.maximum_sentence_length):\n",
    "      self.nb_possible_sentences += (self.vocab_size)**(pos+1)\n",
    "    sentenceId2sentence = np.zeros( (self.nb_possible_sentences, self.maximum_sentence_length))\n",
    "    idx = 1\n",
    "    local_token_pointer = 0\n",
    "    global_token_pointer = 0\n",
    "    while idx != self.nb_possible_sentences:\n",
    "      sentenceId2sentence[idx] = sentenceId2sentence[idx-1]\n",
    "      sentenceId2sentence[idx][local_token_pointer] = (sentenceId2sentence[idx][local_token_pointer]+1)%(self.vocab_size+1)\n",
    "      \n",
    "      while sentenceId2sentence[idx][local_token_pointer] == 0:\n",
    "        # remove the possibility of an empty symbol on the left of actual tokens:\n",
    "        sentenceId2sentence[idx][local_token_pointer] += 1\n",
    "        local_token_pointer += 1\n",
    "        sentenceId2sentence[idx][local_token_pointer] = (sentenceId2sentence[idx][local_token_pointer]+1)%(self.vocab_size+1)\n",
    "      idx += 1\n",
    "      local_token_pointer = 0\n",
    "    \n",
    "    self.sentenceId2sentence = sentenceId2sentence\n",
    "  \n",
    "  def _get_message_from_sentence(self, sentence):\n",
    "    message = ''\n",
    "    for pos, sidx in enumerate(sentence):\n",
    "        # if empty symbol, then there is nothing on the right of it:\n",
    "        if sidx == 0: \n",
    "          # if empty sentence:\n",
    "          if pos == 0:\n",
    "            message = None\n",
    "          break\n",
    "        token = self.id2token[sidx]\n",
    "        message += token\n",
    "    \n",
    "    return message\n",
    "\n",
    "  def step(self, action):\n",
    "    if not self.action_space.contains(action):\n",
    "      raise ValueError('action {} is invalid for {}'.format(action, self.action_space))\n",
    "    \n",
    "    if action != (self.nb_possible_actions-1):\n",
    "      original_action_direction_id = action // self.nb_possible_sentences\n",
    "      original_action_direction = self.actionId2action[original_action_direction_id]\n",
    "    \n",
    "      original_action_sentence_id = (action % self.nb_possible_sentences)\n",
    "      original_action_sentence = self.sentenceId2sentence[original_action_sentence_id]\n",
    "      original_action_message = self._get_message_from_sentence(original_action_sentence)\n",
    "    else:\n",
    "      original_action_direction = \"SKIP\"\n",
    "      original_action_message = None #self.sentenceId2sentence[0] #empty message.\n",
    "    \n",
    "    print(f'discrete action {action} -> original action: direction={original_action_direction} / message={original_action_message}')\n",
    "    \n",
    "    return self.env.step(action=original_action_direction, message=original_action_message)\n",
    "\n",
    "  def is_action_available(self, action):\n",
    "    available = False\n",
    "    if not self.action_space.contains(action):\n",
    "      raise ValueError('action {} is invalid for {}'.format(action, self.action_space))\n",
    "    \n",
    "    if action != (self.nb_possible_actions-1):\n",
    "      original_action_direction_id = action // self.nb_possible_sentences\n",
    "      original_action_direction = self.actionId2action[original_action_direction_id]\n",
    "    else:\n",
    "      original_action_direction = \"SKIP\"\n",
    "     \n",
    "    available = original_action_direction in self.env.action_space\n",
    "    return available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CoMazeGym()\n",
    "wrapped_env = CoMazeGymActionWrapper(env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wrapped_env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5y_ulJCCM-Bi"
   },
   "outputs": [],
   "source": [
    "# Random Agent with Discrete action wrapper\n",
    "obs = wrapped_env.reset()\n",
    "game_over = False\n",
    "while not game_over:\n",
    "  obs, reward, game_over, info = wrapped_env.step(wrapped_env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Agent with Discrete Action Space (Directions+Messages):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CoMazeGym()\n",
    "wrapped_env = CoMazeGymActionWrapper(env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic RL agent\n",
    "# single-layer NN that takes in partial observation of the current state (no walls)\n",
    "# and learns actions WITH communication.\n",
    "\n",
    "nb_possible_actions = wrapped_env.action_space.n \n",
    "ACTION_SPACE = np.arange(nb_possible_actions)\n",
    "\n",
    "class CommRLAgent(nn.Module):\n",
    "  def __init__(self, arena_size, num_actions=1+4*10):\n",
    "    super().__init__()\n",
    "    arena_size_flat = arena_size[0] * arena_size[1]\n",
    "    embed_state_size = 128\n",
    "    self.embed_state = nn.Linear(arena_size_flat,embed_state_size)\n",
    "    embed_action_size = 128\n",
    "    self.embed_action_space = nn.Linear(num_actions,embed_action_size)\n",
    "    policy_input_size = embed_state_size+embed_action_size\n",
    "    self.policy = nn.Linear(policy_input_size,num_actions)\n",
    "\n",
    "  def forward(self, state, action_space):\n",
    "    state_emb = self.embed_state(state)\n",
    "    action_emb = self.embed_action_space(action_space)\n",
    "    state_action_emb = torch.cat((state_emb, action_emb), dim=1)\n",
    "    return self.policy(state_action_emb)\n",
    "\n",
    "\n",
    "def get_state_tensor(obs):\n",
    "  arena_size = (obs['config']['arenaSize']['x'], obs['config']['arenaSize']['y'])\n",
    "  state_tensor = torch.zeros(arena_size).float()\n",
    "  state_tensor[obs['agentPosition']['x']][obs['agentPosition']['y']] = 1    # agent\n",
    "\n",
    "  for goal in obs['unreachedGoals']:\n",
    "    state_tensor[goal['position']['x']][goal['position']['y']] = 2\n",
    "  \n",
    "  return state_tensor\n",
    "\n",
    "\n",
    "def calculate_returns(rewards, discount_factor, normalize = True):\n",
    "    returns = []\n",
    "    R = 0\n",
    "    \n",
    "    for r in reversed(rewards):\n",
    "        R = r + R * discount_factor\n",
    "        returns.insert(0, R)\n",
    "        \n",
    "    returns = torch.tensor(returns)\n",
    "    \n",
    "    if normalize:\n",
    "        returns = (returns - returns.mean()) / returns.std()\n",
    "\n",
    "    return returns\n",
    "\n",
    "\n",
    "discount_factor = 0.9\n",
    "learning_rate = 1e-2\n",
    "num_episodes = 1\n",
    "\n",
    "# arena_size = (obs['arenaSize']['x'], obs['arenaSize']['y'])\n",
    "arena_size = (7,7)\n",
    "agent = CommRLAgent(arena_size, num_actions=nb_possible_actions)\n",
    "optimizer = torch.optim.SGD(agent.parameters(), lr=learning_rate)\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "  obs = env.reset()\n",
    "  \n",
    "  nb_available_actions = 1+2*(wrapped_env.vocab_size**wrapped_env.maximum_sentence_length+1)\n",
    "  action_space_list = [1 if wrapped_env.is_action_available(action_id) else 0 for action_id in ACTION_SPACE]\n",
    "  action_space_tensor = torch.FloatTensor(action_space_list)\n",
    "  action_space_tensor_batch = action_space_tensor.unsqueeze(0)\n",
    "  assert action_space_tensor_batch.sum() == nb_available_actions\n",
    "  \n",
    "  done = False\n",
    "  log_prob_actions = []\n",
    "  rewards = []\n",
    "  episode_reward = 0\n",
    "\n",
    "  while not done:\n",
    "    state_tensor = get_state_tensor(obs)\n",
    "    state_tensor_batch = torch.flatten(state_tensor).unsqueeze(0)\n",
    "    action_pred = agent(state_tensor_batch, action_space_tensor_batch)\n",
    "    \n",
    "    action_prob = F.softmax(action_pred, dim = -1)  \n",
    "    avail_action_prob = action_prob * action_space_tensor\n",
    "    dist = distributions.Categorical(avail_action_prob)\n",
    "    action = dist.sample()\n",
    "    log_prob_action = dist.log_prob(action)\n",
    "\n",
    "    obs, reward, done, _ = wrapped_env.step(action.item())\n",
    "\n",
    "    log_prob_actions.append(log_prob_action)\n",
    "    rewards.append(reward)\n",
    "\n",
    "    episode_reward += reward\n",
    "\n",
    "\n",
    "  log_prob_actions = torch.cat(log_prob_actions)\n",
    "  returns = calculate_returns(rewards, discount_factor).detach()\n",
    "  loss = - (returns * log_prob_actions).sum()\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  print(f'Loss {loss} EP reward {episode_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionnary Observation Space and Discrete Action Space:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionnary contains a 'encoded_pov' that encodes the state of the game similarly to the [BabyAI environment](https://github.com/mila-iqia/babyai).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Dict, MultiDiscrete, Box, Discrete, MultiBinary\n",
    "import numpy as np\n",
    "\n",
    "class CoMazeGymDictObsActionWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, env, vocab_size=10, maximum_sentence_length=1, options={}):\n",
    "        super(CoMazeGymDictObsActionWrapper, self).__init__(env)\n",
    "        self.game = self.env.reset()\n",
    "        \n",
    "        self.nb_directions = 4\n",
    "        self.actionId2action =  [\"LEFT\", \"RIGHT\", \"UP\", \"DOWN\"]\n",
    "        self.action2actionId =  {\"LEFT\":0, \"RIGHT\":1, \"UP\":2, \"DOWN\":3}\n",
    "    \n",
    "        self.vocab_size = vocab_size\n",
    "        assert self.vocab_size == 10\n",
    "        self.token2id = {\n",
    "            \"empty\":0, \n",
    "            \"Q\":1, \n",
    "            \"W\":2, \n",
    "            \"E\":3, \n",
    "            \"R\":4, \n",
    "            \"T\":5, \n",
    "            \"Y\":6, \n",
    "            \"U\":7, \n",
    "            \"I\":8, \n",
    "            \"O\":9, \n",
    "            \"P\":10\n",
    "        }\n",
    "        self.id2token = {\n",
    "            0:\"empty\", \n",
    "            1:\"Q\", \n",
    "            2:\"W\", \n",
    "            3:\"E\", \n",
    "            4:\"R\", \n",
    "            5:\"T\", \n",
    "            6:\"Y\", \n",
    "            7:\"U\", \n",
    "            8:\"I\", \n",
    "            9:\"O\", \n",
    "            10:\"P\"\n",
    "        }\n",
    "        self.maximum_sentence_length = maximum_sentence_length\n",
    "        assert self.maximum_sentence_length == 1\n",
    "        self._build_sentenceId2sentence()\n",
    "        \n",
    "        # Action Space:\n",
    "        self.nb_possible_actions = self.nb_directions*self.nb_possible_sentences+1 \n",
    "        # +1 accounts for the SKIP action...\n",
    "        self.action_space = Discrete(self.nb_possible_actions)\n",
    "        \n",
    "        # Observation Space:\n",
    "        ## previous_message_space\n",
    "        previous_message_space = MultiDiscrete(\n",
    "            [self.vocab_size+1 for _ in range(self.maximum_sentence_length)]\n",
    "        )\n",
    "        ## encoded_pov_space: the depth channel is a one-hot encoding of the tile nature:\n",
    "        self.tile2id = {}\n",
    "        self.nb_different_tile = 3       #background, time bonus, and agent\n",
    "        self.tile2id[\"background\"]= 0\n",
    "        self.tile2id[\"agent\"]= 1\n",
    "        self.tile2id[\"time_bonus\"]= 2\n",
    "        self.nb_different_tile += 4+1    #4 unreached goals + 1 reached goal.\n",
    "        self.goalEnum2id = {\"RED\":3, \"BLUE\":4, \"GREEN\":5, \"YELLOW\":6, 'reached_goal':7}\n",
    "        self.tile2id[\"goal_1\"]= 3\n",
    "        self.tile2id[\"goal_2\"]= 4\n",
    "        self.tile2id[\"goal_3\"]= 5\n",
    "        self.tile2id[\"goal_4\"]= 6\n",
    "        self.tile2id[\"reached_goal\"]= 7\n",
    "        self.nb_different_tile += 4      # wall in any of the 4 directions.\n",
    "        self.wallDirectionEnum2id = {\"LEFT\":8, \"RIGHT\":9, \"UP\":10, \"DOWN\":11}\n",
    "        self.tile2id[\"wall_left\"]= 8\n",
    "        self.tile2id[\"wall_right\"]= 9\n",
    "        self.tile2id[\"wall_up\"]= 10\n",
    "        self.tile2id[\"wall_down\"]= 11\n",
    "        \n",
    "        encoded_pov_space = Box(\n",
    "            low=0, \n",
    "            high=1, \n",
    "            shape=(\n",
    "                self.game[\"config\"][\"arenaSize\"][\"x\"],\n",
    "                self.game[\"config\"][\"arenaSize\"][\"y\"],\n",
    "                self.nb_different_tile\n",
    "            ),\n",
    "            dtype=np.int64, \n",
    "        )\n",
    "        \n",
    "        ## available_action_space:\n",
    "        available_actions_space = MultiBinary(n=self.nb_possible_actions)\n",
    "        \n",
    "        ##\n",
    "        \n",
    "        self.observation_space = Dict({\n",
    "          'encoded_pov': encoded_pov_space,\n",
    "          'available_actions': available_actions_space,\n",
    "          'previous_message': previous_message_space,\n",
    "        })\n",
    "\n",
    "    def _build_sentenceId2sentence(self):\n",
    "        self.nb_possible_sentences = 1 # account for the empty string:\n",
    "        for pos in range(self.maximum_sentence_length):\n",
    "            self.nb_possible_sentences += (self.vocab_size)**(pos+1)\n",
    "        sentenceId2sentence = np.zeros( (self.nb_possible_sentences, self.maximum_sentence_length))\n",
    "        idx = 1\n",
    "        local_token_pointer = 0\n",
    "        global_token_pointer = 0\n",
    "        while idx != self.nb_possible_sentences:\n",
    "            sentenceId2sentence[idx] = sentenceId2sentence[idx-1]\n",
    "            sentenceId2sentence[idx][local_token_pointer] = (sentenceId2sentence[idx][local_token_pointer]+1)%(self.vocab_size+1)\n",
    "            \n",
    "            while sentenceId2sentence[idx][local_token_pointer] == 0:\n",
    "                # remove the possibility of an empty symbol on the left of actual tokens:\n",
    "                sentenceId2sentence[idx][local_token_pointer] += 1\n",
    "                local_token_pointer += 1\n",
    "                sentenceId2sentence[idx][local_token_pointer] = (sentenceId2sentence[idx][local_token_pointer]+1)%(self.vocab_size+1)\n",
    "            idx += 1\n",
    "            local_token_pointer = 0    \n",
    "        \n",
    "        self.sentenceId2sentence = sentenceId2sentence\n",
    "  \n",
    "    def _get_message_from_sentence(self, sentence):\n",
    "        message = ''\n",
    "        for pos, sidx in enumerate(sentence):\n",
    "            # if empty symbol, then there is nothing on the right of it:\n",
    "            if sidx == 0: \n",
    "                # if empty sentence:\n",
    "                if pos == 0:\n",
    "                    message = None\n",
    "                    break\n",
    "            token = self.id2token[sidx]\n",
    "            message += token\n",
    "        return message\n",
    "    \n",
    "    def reset(self, options={}):\n",
    "        level = options.get(\"level\", \"1\")\n",
    "        num_of_player_slots = options.get(\"num_of_player_slots\", \"2\")\n",
    "        self.game_id = requests.post(self.API_URL + \"/game/create?level=\" + level + \"&numOfPlayerSlots=\" + num_of_player_slots).json()[\"uuid\"]\n",
    "        options[\"game_id\"] = self.game_id\n",
    "        \n",
    "        self.game = self.play_existing_game(options)\n",
    "        \n",
    "        self.obs = {}\n",
    "        self.obs[\"encoded_pov\"] = self._encode_game(game=self.game)\n",
    "        \n",
    "        self.obs[\"available_actions\"] = self._get_available_actions(game=self.game)\n",
    "        \n",
    "        self.obs[\"previous_message\"] = np.zeros(self.maximum_sentence_length, dtype=np.int64) #self._get_previous_message(game=self.game)\n",
    "        \n",
    "        return self.obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        if not self.action_space.contains(action):\n",
    "            raise ValueError('action {} is invalid for {}'.format(action, self.action_space))\n",
    "\n",
    "        if action != (self.nb_possible_actions-1):\n",
    "            original_action_direction_id = action // self.nb_possible_sentences\n",
    "            original_action_direction = self.actionId2action[original_action_direction_id]\n",
    "            \n",
    "            original_action_sentence_id = (action % self.nb_possible_sentences)\n",
    "            original_action_sentence = self.sentenceId2sentence[original_action_sentence_id]\n",
    "            original_action_message = self._get_message_from_sentence(original_action_sentence)\n",
    "        else:\n",
    "            original_action_direction = \"SKIP\"\n",
    "            original_action_message = None #self.sentenceId2sentence[0] #empty message.\n",
    "            \n",
    "        print(f'discrete action {action} -> original action: direction={original_action_direction} / message={original_action_message}')\n",
    "\n",
    "        self.game, self.reward, self.done, self.infos = self.env.step(action=original_action_direction, message=original_action_message)\n",
    "        \n",
    "        self.obs = {}\n",
    "        self.obs[\"encoded_pov\"] = self._encode_game(game=self.game)\n",
    "        \n",
    "        self.obs[\"available_actions\"] = self._get_available_actions(game=self.game)\n",
    "        \n",
    "        self.obs[\"previous_message\"] = self._get_previous_message(game=self.game)\n",
    "        import ipdb; ipdb.set_trace()\n",
    "        \n",
    "        return self.obs, self.reward, self.done, self.infos\n",
    "    \n",
    "    def is_action_available(self, action):\n",
    "        available = False\n",
    "        if not self.action_space.contains(action):\n",
    "          raise ValueError('action {} is invalid for {}'.format(action, self.action_space))\n",
    "\n",
    "        if action != (self.nb_possible_actions-1):\n",
    "          original_action_direction_id = action // self.nb_possible_sentences\n",
    "          original_action_direction = self.actionId2action[original_action_direction_id]\n",
    "        else:\n",
    "          original_action_direction = \"SKIP\"\n",
    "\n",
    "        available = original_action_direction in self.env.action_space\n",
    "        return available\n",
    "    \n",
    "    def _encode_game(self, game):\n",
    "        grid = np.zeros(\n",
    "            (game[\"config\"][\"arenaSize\"][\"x\"], game[\"config\"][\"arenaSize\"][\"y\"], self.nb_different_tile),\n",
    "            dtype=np.int64,\n",
    "        )\n",
    "        for x in range(grid.shape[0]):\n",
    "            for y in range(grid.shape[1]):\n",
    "                grid[x][y][0] = 1\n",
    "        \n",
    "        # Agent:\n",
    "        agent_x = game[\"agentPosition\"][\"x\"]\n",
    "        agent_y = game[\"agentPosition\"][\"y\"]\n",
    "        grid[agent_x, agent_y, 0] = 0\n",
    "        grid[agent_x, agent_y, self.tile2id[\"agent\"]] = 1\n",
    "        \n",
    "        # Goals:\n",
    "        goals = game[\"config\"][\"goals\"]\n",
    "        unreached_goals = game[\"unreachedGoals\"]\n",
    "        for goal in goals:\n",
    "            gx, gy = goal[\"position\"][\"x\"], goal[\"position\"][\"y\"]\n",
    "            goal_id = self.goalEnum2id[goal[\"color\"]]\n",
    "            if goal in unreached_goals:\n",
    "                goal_id = self.goalEnum2id[\"reached_goal\"]\n",
    "            grid[gx, gy, 0] = 0\n",
    "            grid[gx, gy, goal_id] = 1\n",
    "        \n",
    "        # Walls?\n",
    "        walls = game[\"config\"][\"walls\"]\n",
    "        for wall in walls:\n",
    "            wx, wy = wall[\"position\"][\"x\"], goal[\"position\"][\"y\"]\n",
    "            wall_id = self.wallDirectionEnum2id[wall[\"direction\"]]\n",
    "            grid[wx, wy, 0] = 0\n",
    "            grid[wx, wy, wall_id] = 1\n",
    "        \n",
    "        return grid\n",
    "    \n",
    "    def _get_available_actions(self, game):\n",
    "        current_player_available_actions = game[\"currentPlayer\"][\"actions\"]\n",
    "        a_actions = np.zeros(self.nb_possible_actions)\n",
    "        # SKIP action:\n",
    "        a_actions[-1] = 1\n",
    "        for action in current_player_available_actions:\n",
    "            if action == \"SKIP\":    continue\n",
    "            action_id = self.action2actionId[action]\n",
    "            for midx in range(self.nb_possible_sentences):\n",
    "                a_actions[action_id*self.nb_possible_sentences+midx] = 1\n",
    "        return a_actions\n",
    "    \n",
    "    def _get_previous_message(self, game):\n",
    "        players = game[\"players\"]\n",
    "        currentPlayer = game[\"currentPlayer\"]\n",
    "        otherPlayers = [player for player in players if player != currentPlayer]\n",
    "        assert len(otherPlayers) == 1\n",
    "        otherPlayer_message = otherPlayers[0][\"lastSymbolMessage\"]\n",
    "        otherPlayer_message_discrete = np.zeros(self.maximum_sentence_length)\n",
    "        if otherPlayer_message is not None:\n",
    "            for widx, token in zip(np.arange(self.maximum_sentence_length), otherPlayer_message):\n",
    "                otherPlayer_message_discrete[widx] = self.token2id[token]\n",
    "        return self._get_message_from_sentence(sentence=otherPlayer_message_discrete)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8fb8gjDM5jP"
   },
   "outputs": [],
   "source": [
    "env = CoMazeGym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictwrapped_env = CoMazeGymDictObsActionWrapper(env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictwrapped_env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictwrapped_env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5y_ulJCCM-Bi"
   },
   "outputs": [],
   "source": [
    "# Random Agent with Dict Observation and Discrete action wrapper\n",
    "%debug\n",
    "obs = dictwrapped_env.reset()\n",
    "print(obs)\n",
    "game_over = False\n",
    "while not game_over:\n",
    "  obs, reward, game_over, info = dictwrapped_env.step(dictwrapped_env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Agent with Dictionary Observation Space and Discrete Action Space (Directions+Messages):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CoMazeGym()\n",
    "wrapped_env = CoMazeGymDictObsActionWrapper(env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Basic RL agent\n",
    "# single-layer NN that takes in the current state as a dictionnary of element.\n",
    "# and learns actions WITH communication.\n",
    "\n",
    "nb_possible_actions = wrapped_env.action_space.n \n",
    "ACTION_SPACE = np.arange(nb_possible_actions)\n",
    "\n",
    "class DictObsCommRLAgent(nn.Module):\n",
    "    def __init__(self, num_actions=1+4*10, pov_shape=[7,7,12], previous_message_length=1):\n",
    "        super().__init__()\n",
    "        pov_embedding = nn.Sequential(\n",
    "            \n",
    "        )\n",
    "    embed_state_size = 128\n",
    "    self.embed_state = nn.Linear(arena_size_flat,embed_state_size)\n",
    "    embed_action_size = 128\n",
    "    self.embed_action_space = nn.Linear(num_actions,embed_action_size)\n",
    "    policy_input_size = embed_state_size+embed_action_size\n",
    "    self.policy = nn.Linear(policy_input_size,num_actions)\n",
    "\n",
    "  def forward(self, state, action_space):\n",
    "    state_emb = self.embed_state(state)\n",
    "    action_emb = self.embed_action_space(action_space)\n",
    "    state_action_emb = torch.cat((state_emb, action_emb), dim=1)\n",
    "    return self.policy(state_action_emb)\n",
    "\n",
    "\n",
    "def get_state_tensor(obs):\n",
    "  arena_size = (obs['config']['arenaSize']['x'], obs['config']['arenaSize']['y'])\n",
    "  state_tensor = torch.zeros(arena_size).float()\n",
    "  state_tensor[obs['agentPosition']['x']][obs['agentPosition']['y']] = 1    # agent\n",
    "\n",
    "  for goal in obs['unreachedGoals']:\n",
    "    state_tensor[goal['position']['x']][goal['position']['y']] = 2\n",
    "  \n",
    "  return state_tensor\n",
    "\n",
    "\n",
    "def calculate_returns(rewards, discount_factor, normalize = True):\n",
    "    returns = []\n",
    "    R = 0\n",
    "    \n",
    "    for r in reversed(rewards):\n",
    "        R = r + R * discount_factor\n",
    "        returns.insert(0, R)\n",
    "        \n",
    "    returns = torch.tensor(returns)\n",
    "    \n",
    "    if normalize:\n",
    "        returns = (returns - returns.mean()) / returns.std()\n",
    "\n",
    "    return returns\n",
    "\n",
    "\n",
    "discount_factor = 0.9\n",
    "learning_rate = 1e-2\n",
    "num_episodes = 1\n",
    "\n",
    "# arena_size = (obs['arenaSize']['x'], obs['arenaSize']['y'])\n",
    "arena_size = (7,7)\n",
    "agent = CommRLAgent(arena_size, num_actions=nb_possible_actions)\n",
    "optimizer = torch.optim.SGD(agent.parameters(), lr=learning_rate)\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "  obs = env.reset()\n",
    "  \n",
    "  nb_available_actions = 1+2*(wrapped_env.vocab_size**wrapped_env.maximum_sentence_length+1)\n",
    "  action_space_list = [1 if wrapped_env.is_action_available(action_id) else 0 for action_id in ACTION_SPACE]\n",
    "  action_space_tensor = torch.FloatTensor(action_space_list)\n",
    "  action_space_tensor_batch = action_space_tensor.unsqueeze(0)\n",
    "  assert action_space_tensor_batch.sum() == nb_available_actions\n",
    "  \n",
    "  done = False\n",
    "  log_prob_actions = []\n",
    "  rewards = []\n",
    "  episode_reward = 0\n",
    "\n",
    "  while not done:\n",
    "    state_tensor = get_state_tensor(obs)\n",
    "    state_tensor_batch = torch.flatten(state_tensor).unsqueeze(0)\n",
    "    action_pred = agent(state_tensor_batch, action_space_tensor_batch)\n",
    "    \n",
    "    action_prob = F.softmax(action_pred, dim = -1)  \n",
    "    avail_action_prob = action_prob * action_space_tensor\n",
    "    dist = distributions.Categorical(avail_action_prob)\n",
    "    action = dist.sample()\n",
    "    log_prob_action = dist.log_prob(action)\n",
    "\n",
    "    obs, reward, done, _ = wrapped_env.step(action.item())\n",
    "\n",
    "    log_prob_actions.append(log_prob_action)\n",
    "    rewards.append(reward)\n",
    "\n",
    "    episode_reward += reward\n",
    "\n",
    "\n",
    "  log_prob_actions = torch.cat(log_prob_actions)\n",
    "  returns = calculate_returns(rewards, discount_factor).detach()\n",
    "  loss = - (returns * log_prob_actions).sum()\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  print(f'Loss {loss} EP reward {episode_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2m8l6uJVk1-"
   },
   "outputs": [],
   "source": [
    "# Basic RL agent\n",
    "# single-layer NN that takes in current state and learns action\n",
    "# import torch\n",
    "\n",
    "# obs = env.reset()\n",
    "# game_over = False\n",
    "# action_space = env.action_space\n",
    "# goals_pos = [goal['position']\n",
    "#              for goal in obs['config']['goals']]\n",
    "\n",
    "\n",
    "# obs, reward, game_over, info = env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXivOcskf347"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "CoMaze Agent Template",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
