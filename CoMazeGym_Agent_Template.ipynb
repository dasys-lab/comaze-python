{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dasys-lab/comaze-python/blob/gym-env/CoMazeGym_Agent_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPALy2GEL_vQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "class LoggingAbstractEnv:\n",
    "  def __init__(self, logging_base_path=\"./logs/games/\"):\n",
    "    self.action2actionId =  {\"LEFT\":0, \"RIGHT\":1, \"UP\":2, \"DOWN\":3, \"SKIP\":4}\n",
    "    self.token2tokenId = {\n",
    "        \"empty\":0, \n",
    "        \"Q\":1, \n",
    "        \"W\":2, \n",
    "        \"E\":3, \n",
    "        \"R\":4, \n",
    "        \"T\":5, \n",
    "        \"Y\":6, \n",
    "        \"U\":7, \n",
    "        \"I\":8, \n",
    "        \"O\":9, \n",
    "        \"P\":10\n",
    "    }\n",
    "    \n",
    "    self.logging_base_path = logging_base_path\n",
    "    self.per_episode_logging_iteration = 0\n",
    "    self.logger = None\n",
    "    \n",
    "  def _init_logger(self):\n",
    "    \"\"\"\n",
    "    Assumes that the inherited class has attribute game_id and player_id.\n",
    "    \"\"\"\n",
    "    self.logging_folder_path = os.path.join(self.logging_base_path, f\"{self.game_id}\")\n",
    "    os.makedirs(self.logging_folder_path, exist_ok=True)\n",
    "    self.logging_log_path = os.path.join(self.logging_folder_path, f\"{self.player_id}.tlog\")\n",
    "    self.logger = SummaryWriter(self.logging_log_path)\n",
    "    self.per_episode_logging_iteration = 0\n",
    "    \n",
    "  def _log(self, action, message, reward):\n",
    "    if message is None:\n",
    "        message = \"empty\"\n",
    "    \n",
    "    self.logger.add_scalar(\"Training/Reward\", reward, self.per_episode_logging_iteration)\n",
    "    \n",
    "    action_id = self.action2actionId[action]\n",
    "    message_id = self.token2tokenId[message]\n",
    "    \n",
    "    print(f\"Logging: action::{action_id} / message::{message_id}.\" )\n",
    "    \n",
    "    self.logger.add_scalar(\"Training/Action\", action_id, self.per_episode_logging_iteration)\n",
    "    self.logger.add_scalar(\"Training/Message\", message_id, self.per_episode_logging_iteration)\n",
    "    \n",
    "    action_hist =  np.zeros(len(self.action2actionId), dtype=np.int64)\n",
    "    action_hist[action_id] = 1\n",
    "    message_hist =  np.zeros(len(self.token2tokenId), dtype=np.int64)\n",
    "    message_hist[message_id] = 1\n",
    "    \n",
    "    self.logger.add_histogram(\"Traning/ActionHist\", action_hist, self.per_episode_logging_iteration)\n",
    "    self.logger.add_histogram(\"Traning/MessageHist\", message_hist, self.per_episode_logging_iteration)\n",
    "    \n",
    "    self.per_episode_logging_iteration += 1\n",
    "    self.logger.flush()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoMazeGym(LoggingAbstractEnv):\n",
    "  if os.path.isfile(\".local\"):\n",
    "    API_URL = \"http://localhost:16216\"\n",
    "    WEBAPP_URL = \"http://localhost\"\n",
    "  else:\n",
    "    API_URL = \"http://teamwork.vs.uni-kassel.de:16216\"\n",
    "    WEBAPP_URL = \"http://teamwork.vs.uni-kassel.de\"\n",
    "  LIB_VERSION = \"1.1.0\"\n",
    "  \n",
    "  def __init__(self, logging_base_path=\"./logs/games/\"):\n",
    "    super(CoMazeGym, self).__init__(logging_base_path=logging_base_path)\n",
    "    self.game = None\n",
    "    self.game_id = None\n",
    "    self.player_id = None\n",
    "    self.action_space = None\n",
    "    \n",
    "  def reset(self, options={}):\n",
    "    level = options.get(\"level\", \"1\")\n",
    "    num_of_player_slots = options.get(\"num_of_player_slots\", \"2\")\n",
    "    self.game_id = requests.post(self.API_URL + \"/game/create?level=\" + level + \"&numOfPlayerSlots=\" + num_of_player_slots).json()[\"uuid\"]\n",
    "    options[\"game_id\"] = self.game_id\n",
    "    return self.play_existing_game(options)\n",
    "\n",
    "  def play_existing_game(self, options={}):\n",
    "    if \"look_for_player_name\" in options:\n",
    "      options[\"game_id\"] = requests.get(self.API_URL + \"/game/byPlayerName?playerName=\" + options[\"look_for_player_name\"]).json()[\"uuid\"]\n",
    "\n",
    "    if \"game_id\" not in options or len(options[\"game_id\"]) != 36:\n",
    "      raise Exception(\"You must provide a game id when attending an existing game. Use play_new_game() instead of play_existing_game() if you want to create a new game.\")\n",
    "\n",
    "    player_name = options.get(\"player_name\", \"Python\")\n",
    "    self.game_id = options[\"game_id\"]\n",
    "    print(\"Joined gameId: \" + self.game_id)\n",
    "    player = requests.post(self.API_URL + \"/game/\" + self.game_id + \"/attend?playerName=\" + player_name).json()\n",
    "    self.player_id = player[\"uuid\"]\n",
    "    self.action_space = player['directions'] + ['SKIP']\n",
    "    print(\"Playing as playerId: \" + self.player_id)\n",
    "    self.game = requests.get(self.API_URL + \"/game/\" + self.game_id).json()\n",
    "    print(f'Action Space is {self.action_space}')\n",
    "    while self.game['currentPlayer']['uuid'] != self.player_id:\n",
    "      print(f'Waiting for other player to make first move')\n",
    "      time.sleep(1)\n",
    "      self.game = requests.get(self.API_URL + \"/game/\" + self.game_id).json()\n",
    "    \n",
    "    self._init_logger()\n",
    "    \n",
    "    return self.game\n",
    "\n",
    "  def step(self, action, message=None):\n",
    "    moved = False\n",
    "    while not moved:\n",
    "      self.game = requests.get(self.API_URL + \"/game/\" + self.game_id).json()\n",
    "\n",
    "      if not self.game[\"state\"][\"started\"]:\n",
    "        print(\"Waiting for players. (Invite someone: \" + self.WEBAPP_URL + \"/?gameId=\" + self.game_id + \")\")\n",
    "        time.sleep(3)\n",
    "        continue\n",
    "\n",
    "      print(\"Moving \" + action)\n",
    "      print(f'Sending message {message}')\n",
    "      print('---')\n",
    "      self.game = requests.post(self.API_URL + \"/game/\" + self.game_id + \"/move?playerId=\" + self.player_id + \"&action=\" + action).json()\n",
    "      moved = True\n",
    "    \n",
    "    if self.game[\"state\"][\"won\"]:\n",
    "      print(\"Game won!\")\n",
    "      reward = 1\n",
    "    elif self.game[\"state\"][\"lost\"]:\n",
    "      print(\"Game lost (\" + self.game[\"state\"][\"lostMessage\"] + \").\")\n",
    "      reward = -1\n",
    "    else:\n",
    "      reward = 0\n",
    "    \n",
    "    if not self.game[\"state\"][\"over\"]:\n",
    "      # wait for other player to make a move before sending back obs\n",
    "      while self.game['currentPlayer']['uuid'] != self.player_id:\n",
    "        print(f'Waiting for other player to make a move')\n",
    "        time.sleep(1)\n",
    "        self.game = requests.get(self.API_URL + \"/game/\" + self.game_id).json()\n",
    "    \n",
    "    self._log(action=action, message=message, reward=reward)\n",
    "    \n",
    "    return self.game, reward, self.game[\"state\"][\"over\"], None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8fb8gjDM5jP",
    "outputId": "d92aaffb-251b-402e-ff26-ceb4cb9034ca"
   },
   "outputs": [],
   "source": [
    "env = CoMazeGym()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5y_ulJCCM-Bi"
   },
   "outputs": [],
   "source": [
    "# Random Agent\n",
    "import random \n",
    "\n",
    "id2token = {\n",
    "    0:\"empty\", \n",
    "    1:\"Q\", \n",
    "    2:\"W\", \n",
    "    3:\"E\", \n",
    "    4:\"R\", \n",
    "    5:\"T\", \n",
    "    6:\"Y\", \n",
    "    7:\"U\", \n",
    "    8:\"I\", \n",
    "    9:\"O\", \n",
    "    10:\"P\"\n",
    "}\n",
    "\n",
    "obs = env.reset()\n",
    "game_over = False\n",
    "while not game_over:\n",
    "  message_id = np.random.randint(low=0, high=11)\n",
    "  message = id2token[message_id]\n",
    "  if message == \"empty\":\n",
    "    message = None\n",
    "  obs, reward, game_over, info = env.step(action=random.choice(env.action_space), message=message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ua-MVU_4PXo3"
   },
   "outputs": [],
   "source": [
    "# Nearest Goal Agent\n",
    "# Choose a nearest goal, see if one of your actions can get you there, if so take that action\n",
    "obs = env.reset()\n",
    "game_over = False\n",
    "action_space = env.action_space\n",
    "goals_pos = [goal['position']\n",
    "             for goal in obs['config']['goals']]\n",
    "\n",
    "while not game_over:\n",
    "  goals_pos = [goal['position'] for goal in obs['unreachedGoals']]\n",
    "  agent_pos = obs['agentPosition']\n",
    "  \n",
    "  goal_diffs = [(goal['x'] - agent_pos['x'], goal['y'] - agent_pos['y'])\n",
    "                for goal in goals_pos]\n",
    "  goal_dists = [abs(diff[0])+abs(diff[1]) for diff in goal_diffs]\n",
    "  nearest_goal = goal_dists.index(min(goal_dists)) \n",
    "\n",
    "  print(f'Nearest goal is {obs[\"unreachedGoals\"][nearest_goal]}')\n",
    "  print(f'Nearest goal diff {goal_diffs[nearest_goal]}')\n",
    "\n",
    "  move_x, move_y = goal_diffs[nearest_goal]\n",
    "\n",
    "  if 'LEFT' in action_space and move_x < 0:\n",
    "    action = 'LEFT'\n",
    "  elif 'RIGHT' in action_space and move_x > 0:\n",
    "    action = 'RIGHT'\n",
    "  elif 'UP' in action_space and move_y < 0:\n",
    "    action = 'UP'\n",
    "  elif 'DOWN' in action_space and move_y > 0:\n",
    "    action = 'DOWN'\n",
    "  else:\n",
    "    action = 'SKIP'\n",
    "\n",
    "  obs, reward, game_over, info = env.step(action)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "piDiUvnQKVOB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "T2m8l6uJVk1-",
    "outputId": "507fb974-3c66-42c9-c313-d7215d581c9a"
   },
   "outputs": [],
   "source": [
    "# Basic RL agent\n",
    "# single-layer NN that takes in current state and learns action\n",
    "\n",
    "ACTION_SPACE = ['LEFT', 'RIGHT', 'UP', 'DOWN', 'SKIP']\n",
    "\n",
    "class RLAgent(nn.Module):\n",
    "  def __init__(self, arena_size, num_actions=5):\n",
    "    super().__init__()\n",
    "    arena_size_flat = arena_size[0] * arena_size[1]\n",
    "    self.embed_state = nn.Linear(arena_size_flat,16)\n",
    "    self.embed_action_space = nn.Linear(5,5)\n",
    "    self.policy = nn.Linear(21,num_actions)\n",
    "\n",
    "  def forward(self, state, action_space):\n",
    "    state_emb = self.embed_state(state)\n",
    "    action_emb = self.embed_action_space(action_space)\n",
    "    state_action_emb = torch.cat((state_emb, action_emb), dim=1)\n",
    "    return self.policy(state_action_emb)\n",
    "\n",
    "\n",
    "def get_state_tensor(obs):\n",
    "  arena_size = (obs['config']['arenaSize']['x'], obs['config']['arenaSize']['y'])\n",
    "  state_tensor = torch.zeros(arena_size).float()\n",
    "  state_tensor[obs['agentPosition']['x']][obs['agentPosition']['y']] = 1    # agent\n",
    "\n",
    "  for goal in obs['unreachedGoals']:\n",
    "    state_tensor[goal['position']['x']][goal['position']['y']] = 2\n",
    "  \n",
    "  return state_tensor\n",
    "\n",
    "\n",
    "def calculate_returns(rewards, discount_factor, normalize = True):\n",
    "    returns = []\n",
    "    R = 0\n",
    "    \n",
    "    for r in reversed(rewards):\n",
    "        R = r + R * discount_factor\n",
    "        returns.insert(0, R)\n",
    "        \n",
    "    returns = torch.tensor(returns)\n",
    "    \n",
    "    if normalize:\n",
    "        returns = (returns - returns.mean()) / returns.std()\n",
    "\n",
    "    return returns\n",
    "\n",
    "\n",
    "action_space_list = [1 if x in env.action_space else 0 for x in ACTION_SPACE]\n",
    "action_space_tensor = torch.FloatTensor(action_space_list)\n",
    "action_space_tensor_batch = action_space_tensor.unsqueeze(0)\n",
    "\n",
    "discount_factor = 0.9\n",
    "learning_rate = 1e-2\n",
    "num_episodes = 1\n",
    "\n",
    "# arena_size = (obs['arenaSize']['x'], obs['arenaSize']['y'])\n",
    "arena_size = (7,7)\n",
    "agent = RLAgent(arena_size)\n",
    "optimizer = torch.optim.SGD(agent.parameters(), lr=learning_rate)\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "  obs = env.reset()\n",
    "\n",
    "  done = False\n",
    "  log_prob_actions = []\n",
    "  rewards = []\n",
    "  episode_reward = 0\n",
    "\n",
    "  while not done:\n",
    "    state_tensor = get_state_tensor(obs)\n",
    "    state_tensor_batch = torch.flatten(state_tensor).unsqueeze(0)\n",
    "    action_pred = agent(state_tensor_batch, action_space_tensor_batch)\n",
    "    \n",
    "    action_prob = F.softmax(action_pred, dim = -1)  \n",
    "    avail_action_prob = action_prob * action_space_tensor\n",
    "    dist = distributions.Categorical(avail_action_prob)\n",
    "    action = dist.sample()\n",
    "    log_prob_action = dist.log_prob(action)\n",
    "\n",
    "    obs, reward, done, _ = env.step(ACTION_SPACE[action.item()])\n",
    "\n",
    "    log_prob_actions.append(log_prob_action)\n",
    "    rewards.append(reward)\n",
    "\n",
    "    episode_reward += reward\n",
    "\n",
    "\n",
    "  log_prob_actions = torch.cat(log_prob_actions)\n",
    "  returns = calculate_returns(rewards, discount_factor).detach()\n",
    "  loss = - (returns * log_prob_actions).sum()\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  print(f'Loss {loss} EP reward {episode_reward}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXivOcskf347"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "CoMazeGym Agent Template",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
