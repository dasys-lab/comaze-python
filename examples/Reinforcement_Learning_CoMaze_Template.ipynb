{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning CoMaze Template",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyEyUoNesM52"
      },
      "source": [
        "# Installing the dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svH1hL1teVFu",
        "outputId": "08129503-0918-4da1-c008-ce7971fd1dab"
      },
      "source": [
        "!git clone https://github.com/Near32/comaze-python.git ; cd comaze-python; git checkout develop-rl-template; git pull; git status; pip install -e ."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'comaze-python' already exists and is not an empty directory.\n",
            "Already on 'develop-rl-template'\n",
            "Your branch is up to date with 'origin/develop-rl-template'.\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 17 (delta 8), reused 17 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (17/17), done.\n",
            "From https://github.com/Near32/comaze-python\n",
            "   52a41aa..4117798  develop-rl-template -> origin/develop-rl-template\n",
            "Updating 52a41aa..4117798\n",
            "Fast-forward\n",
            " comaze/agents/abstract_agent.py                    |  14 \u001b[32m++\u001b[m\u001b[31m-\u001b[m\n",
            " comaze/agents/rl/abstract_on_policy_rl_agent.py    |  54 \u001b[32m++++++\u001b[m\u001b[31m----\u001b[m\n",
            " comaze/agents/rl/simple_on_policy_rl_agent.py      |  11 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " comaze/env/comaze.py                               |  24 \u001b[32m+++\u001b[m\u001b[31m--\u001b[m\n",
            " setup.py                                           |  17 \u001b[32m++\u001b[m\u001b[31m--\u001b[m\n",
            " .../rl/test_training_simple_on_policy_rl_agent.py  | 111 \u001b[32m+++++++++++++++++++++\u001b[m\n",
            " .../test_dict_encoded_pov_avail_move_exp_utils.py  |   1 \u001b[32m+\u001b[m\n",
            " 7 files changed, 184 insertions(+), 48 deletions(-)\n",
            " create mode 100644 tests/agents/rl/test_training_simple_on_policy_rl_agent.py\n",
            "On branch develop-rl-template\n",
            "Your branch is up to date with 'origin/develop-rl-template'.\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31mcomaze.egg-info/\u001b[m\n",
            "\t\u001b[31mcomaze/__pycache__/\u001b[m\n",
            "\t\u001b[31mcomaze/agents/__pycache__/\u001b[m\n",
            "\t\u001b[31mcomaze/agents/rl/__pycache__/\u001b[m\n",
            "\t\u001b[31mcomaze/agents/utils/__pycache__/\u001b[m\n",
            "\t\u001b[31mcomaze/env/__pycache__/\u001b[m\n",
            "\t\u001b[31mcomaze/utils/__pycache__/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n",
            "Obtaining file:///content/comaze-python\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from comaze==0.0.1) (4.41.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from comaze==0.0.1) (0.17.3)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.6/dist-packages (from comaze==0.0.1) (2.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from comaze==0.0.1) (1.7.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from comaze==0.0.1) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->comaze==0.0.1) (1.19.4)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->comaze==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->comaze==0.0.1) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->comaze==0.0.1) (1.5.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx->comaze==0.0.1) (3.14.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardx->comaze==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch->comaze==0.0.1) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->comaze==0.0.1) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->comaze==0.0.1) (8.0.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->comaze==0.0.1) (0.18.2)\n",
            "Installing collected packages: comaze\n",
            "  Found existing installation: comaze 0.0.1\n",
            "    Can't uninstall 'comaze'. No files were found to uninstall.\n",
            "  Running setup.py develop for comaze\n",
            "Successfully installed comaze\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaD7n1EnsTHP"
      },
      "source": [
        "# Before continuing any further, please restart the kernel (Runtime->restart runtime) in order to make the installed packaged available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYIAV-HgoHio"
      },
      "source": [
        "# 1) Create a simple (non-communicating, but coordinating) On-Policy RL Agent:\n",
        "\n",
        "## What does the player know and see about the game?\n",
        "\n",
        "Taking a look at the observation space of the game CoMaze will allow us to understand what will our player see at each step. The observation space is comprised of the following elements:\n",
        "\n",
        "\n",
        "*   an arena/game board (giving us the position of the agent, among other things),\n",
        "*   the list of directional moves available to the current player (among [\"LEFT\", \"RIGHT\", \"UP\", \"DOWN\", \"SKIP\"]),\n",
        "*   the last message coming from the other player (if any),\n",
        "*   and the secret rule of the current player (specifying whether to reach a given-color goal before that of another color).\n",
        "\n",
        "In this first example, we will only care about the arena/game board and the list of directional moves that are available to the current player.\n",
        "\n",
        "### RL-based player's observation space:\n",
        "\n",
        "In order to make it possible for a (deep) RL-based player to make sense of arena/game board and possible directional moves, those have been pre-formatted and will be delivered at each step to your player via a Dict structure containing the following:\n",
        "\n",
        "*    the arena/game board is stored in the key \"encoded_pov\" and takes the shape of a 3D tensor of size 7x7x12. The arena/game board is indeed 7 tile-large in width and 7 tile-long in height. Each entry in that tensor corresponds to the nature of the corresponding tile. \n",
        "*    the list of directional moves available to the current player is stored in the key \"available_moves\". It is represented by a 1D vector of size 5. Entry i contains a 1 if the i-th directional move (amongamong [\"LEFT\", \"RIGHT\", \"UP\", \"DOWN\", \"SKIP\"]) is available to the current player. \n",
        "\n",
        "As we will see below in the example, the \"encoded_pov\" will be dealt with a CNN while the \"available_moves\" will be dealt with a fully-connected layer...\n",
        "\n",
        "\n",
        "## Action Space: What moves can the player do?\n",
        "\n",
        "At each step, the player can execute a move that consist of choosing the following two elements:\n",
        "\n",
        "*    a directional move (among [\"LEFT\", \"RIGHT\", \"UP\", \"DOWN\", \"SKIP\"]),\n",
        "*    and a message (among [EMPTY_MESSAGE, \"Q\", \"W\", \"E\", \"R\", T\", \"Y\", \"U\", \"I\", \"O\", \"P\"]).\n",
        "\n",
        "When the agent chooses the \"SKIP\" directional move, then the game forces the message to be the EMPTY_MESSAGE, i.e. no message will be passed onto the next player. \n",
        "\n",
        "(Note that, in the current version of the game, we only allow a vocabulary size of V=10 and a maximum sentence length of L=1.)\n",
        "\n",
        "In this first example, we will only care about the directional move, thus focusing on our player's ability to coordinate with each other without any communication.\n",
        "\n",
        "As we will see below, the player is expected to output a discrete action/move id from the range [0,5].\n",
        "\n",
        "## Template: Have a go at modifying it!\n",
        "\n",
        "Below is the template of an RL-based player.\n",
        "Please have a go at modifying the architecture of its neural network, in the method build_model.\n",
        "Be careful to accomodate any change made in the network topography in the select_action method.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFSASY19FLsH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe95Vt3loOL6"
      },
      "source": [
        "from typing import Any\n",
        "from typing import Dict\n",
        "from typing import List\n",
        "from typing import Callable\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np \n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import distributions \n",
        "\n",
        "from comaze.agents.rl import AbstractOnPolicyRLAgent\n",
        "from comaze.agents.utils import dict_encoded_pov_avail_moves_extract_exp_fn, discrete_direction_only_format_move_fn\n",
        "\n",
        "\n",
        "class SimpleOnPolicyRLAgent(AbstractOnPolicyRLAgent):\n",
        "  \"\"\"\n",
        "  Simple on-policy RL agents using PyTorch.\n",
        "  \n",
        "  Call init_rl_algo at the end of the init function.\n",
        "\n",
        "  The output of select_action must be a dictionnary containing:\n",
        "    - \"action\": the actual action that needs to be transformed \n",
        "                using the format_move_fn function.\n",
        "    - \"log_prob_action\": the log likelihood over the action\n",
        "                          distribution. \n",
        "  \n",
        "  Note the default extract_exp_fn and format_move_fn functions.\n",
        "  They are the minimum to allow any learning to take place.\n",
        "\n",
        "  As AbstractAgent requests it, you also need to implement:\n",
        "    - agent_id: Agent's unique id.\n",
        "    - select_action: Agent's action selection logic.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "    self, \n",
        "    learning_rate: float=1e-4,\n",
        "    discount_factor: float=0.99,\n",
        "    num_actions: int=5,\n",
        "    pov_shape: List[int]=[7,7,12],\n",
        "    use_cuda: Optional[Any]=False,\n",
        "    ) -> None:\n",
        "    \"\"\"\n",
        "    Initializes the agent.\n",
        "    \"\"\"\n",
        "    nn.Module.__init__(self=self)\n",
        "    AbstractOnPolicyRLAgent.__init__(\n",
        "      self=self,\n",
        "      extract_exp_fn=dict_encoded_pov_avail_moves_extract_exp_fn, \n",
        "      format_move_fn=discrete_direction_only_format_move_fn,\n",
        "      learning_rate=learning_rate,\n",
        "      discount_factor=discount_factor,\n",
        "    )\n",
        "\n",
        "    self.num_actions = num_actions\n",
        "    self.pov_shape = pov_shape\n",
        "    self.use_cuda = use_cuda\n",
        "    self.build_agent()\n",
        "    \n",
        "    self.init_rl_algo()\n",
        "  \n",
        "  @property\n",
        "  def agent_id(self) -> str:\n",
        "    #################\n",
        "    ## MODIFY HERE ##\n",
        "    return \"simple_onpolicy_rlagent\"\n",
        "    ## MODIFY HERE ##\n",
        "    #################\n",
        "    \n",
        "  def build_agent(self):\n",
        "    #################\n",
        "    ## MODIFY HERE ##\n",
        "    self.embed_pov_size = 256\n",
        "    self.embed_pov = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=self.pov_shape[-1], out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(512, self.embed_pov_size),\n",
        "      nn.ReLU(),\n",
        "    )\n",
        "    \n",
        "    self.embed_action_size = 128\n",
        "    self.embed_action_space = nn.Linear(self.num_actions, self.embed_action_size)\n",
        "    \n",
        "    policy_input_size = self.embed_pov_size+self.embed_action_size\n",
        "    self.policy = nn.Linear(policy_input_size, self.num_actions)\n",
        "    ## MODIFY HERE ##\n",
        "    #################\n",
        "\n",
        "    if self.use_cuda:\n",
        "      self.cuda()\n",
        "\n",
        "  def get_formatted_inputs(self, obs):\n",
        "    nobs = {}\n",
        "    for k,v in obs.items():\n",
        "      if 'pov' in k:\n",
        "        # move channels around:\n",
        "        assert len(v.shape)==3\n",
        "        v = np.transpose(v, (2,0,1))\n",
        "      nv = torch.from_numpy(v).unsqueeze(0).float()\n",
        "      nobs[k] = nv.cuda() if self.use_cuda else nv\n",
        "    return nobs\n",
        "\n",
        "  def select_action(self, observation: Any) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Returns agent's action given `observation`.\n",
        "    \"\"\"\n",
        "\n",
        "    obs = self.get_formatted_inputs(observation)\n",
        "\n",
        "    pov_input = obs[\"encoded_pov\"]\n",
        "    action_space = obs[\"available_moves\"]\n",
        "    \n",
        "    #################\n",
        "    ## MODIFY HERE ##\n",
        "    pov_emb = self.embed_pov(pov_input)\n",
        "    action_emb = self.embed_action_space(action_space)\n",
        "    \n",
        "    pov_action_emb = torch.cat((pov_emb, action_emb), dim=1)\n",
        "    action_pred = self.policy(pov_action_emb)\n",
        "    ## MODIFY HERE ##\n",
        "    #################\n",
        "\n",
        "    action_prob = F.softmax(action_pred, dim = -1)  \n",
        "    avail_action_prob = action_prob * obs[\"available_moves\"]\n",
        "    dist = distributions.Categorical(avail_action_prob)\n",
        "    action = dist.sample()\n",
        "    log_prob_action = dist.log_prob(action)\n",
        "\n",
        "    action_dict = {\n",
        "      \"action\": action.item(),\n",
        "      \"log_prob_action\": log_prob_action\n",
        "    }\n",
        "\n",
        "    return action_dict"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so6LR4Yqe5Mw"
      },
      "source": [
        "import random\n",
        "from typing import Callable\n",
        "import pandas as pd \n",
        "\n",
        "from functools import partial\n",
        "from tqdm import tqdm \n",
        "from tensorboardX import SummaryWriter \n",
        "\n",
        "from comaze.env import TwoPlayersCoMazeGym\n",
        "from comaze.agents import AbstractAgent, SimpleOnPolicyRLAgent\n",
        "\n",
        "\n",
        "def two_players_environment_loop(\n",
        "    agent1: AbstractAgent,\n",
        "    agent2: AbstractAgent,\n",
        "    environment,\n",
        "    max_episode_length,\n",
        "):\n",
        "  \"\"\"\n",
        "  Loop runner for the environment.\n",
        "  \"\"\"\n",
        "  # Setup environment.\n",
        "  state = environment.reset()\n",
        "\n",
        "  # Book-keeping.\n",
        "  t = 0\n",
        "  done = False\n",
        "  trajectory = list()\n",
        "  cum_reward = 0\n",
        "\n",
        "  ebar = tqdm(total=max_episode_length, position=1)\n",
        "  while not done and t<=max_episode_length:\n",
        "    ebar.update(1)\n",
        "    # Turn-based game.\n",
        "    if t%2 == 0:\n",
        "      move = agent1.select_move(state)\n",
        "    else:\n",
        "      move = agent2.select_move(state)\n",
        "  \n",
        "    # Progress simulation.\n",
        "    next_state, reward, done, info = environment.step(move)\n",
        "\n",
        "    if t==max_episode_length:\n",
        "      done = True\n",
        "      reward += -1\n",
        "\n",
        "    for agent in [agent1, agent2]:\n",
        "      agent.update(move, next_state, reward, done)\n",
        "\n",
        "    # Book-keeping.\n",
        "    trajectory.append((t, state, move, reward, next_state, done, info))\n",
        "\n",
        "    cum_reward += reward\n",
        "    t = t + 1\n",
        "    state = next_state\n",
        "  \n",
        "\n",
        "  # Dump logs.\n",
        "  pd.DataFrame(trajectory).to_csv(\"{}-{}.csv\".format(\n",
        "      agent1.agent_id, agent2.agent_id)\n",
        "  )\n",
        "\n",
        "  return cum_reward, trajectory"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDP6P3cJ39rd"
      },
      "source": [
        "## Let us test the players:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLO42an0fO2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad28d20b-461e-43ff-ea81-27d94bfaeb07"
      },
      "source": [
        "use_cuda = True # make sure to use a GPU in Runtime->change runtime type... \n",
        "sparse_reward = False\n",
        "\n",
        "agent1 = SimpleOnPolicyRLAgent( \n",
        "  learning_rate=1e-4,\n",
        "  discount_factor=0.99,\n",
        "  num_actions=5,\n",
        "  pov_shape=[7,7,12],\n",
        "  use_cuda=use_cuda,\n",
        ")\n",
        "\n",
        "agent2 = SimpleOnPolicyRLAgent( \n",
        "  learning_rate=1e-4,\n",
        "  discount_factor=0.99,\n",
        "  num_actions=5,\n",
        "  pov_shape=[7,7,12],\n",
        "  use_cuda=use_cuda,\n",
        ")\n",
        "\n",
        "max_episode_length = 50\n",
        "verbose = False \n",
        "\n",
        "environment_kwargs = {\n",
        "    \"level\":\"1\",\n",
        "    \"verbose\":verbose,\n",
        "}\n",
        "\n",
        "environment = TwoPlayersCoMazeGym(**environment_kwargs)\n",
        "\n",
        "two_players_environment_loop(\n",
        "    agent1=agent1,\n",
        "    agent2=agent2,\n",
        "    environment=environment,\n",
        "    max_episode_length=max_episode_length,\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4%|▍         | 2/50 [00:00<00:02, 16.63it/s]\u001b[A\n",
            "  6%|▌         | 3/50 [00:00<00:03, 12.66it/s]\u001b[A\n",
            "  8%|▊         | 4/50 [00:00<00:04, 10.62it/s]\u001b[A\n",
            " 10%|█         | 5/50 [00:00<00:04,  9.76it/s]\u001b[A\n",
            " 12%|█▏        | 6/50 [00:00<00:04,  9.23it/s]\u001b[A\n",
            " 14%|█▍        | 7/50 [00:00<00:04,  8.81it/s]\u001b[A\n",
            " 16%|█▌        | 8/50 [00:00<00:04,  8.57it/s]\u001b[A\n",
            " 18%|█▊        | 9/50 [00:00<00:04,  8.38it/s]\u001b[A\n",
            " 20%|██        | 10/50 [00:01<00:04,  8.28it/s]\u001b[A\n",
            " 22%|██▏       | 11/50 [00:01<00:04,  8.26it/s]\u001b[A\n",
            " 24%|██▍       | 12/50 [00:01<00:04,  8.29it/s]\u001b[A\n",
            " 26%|██▌       | 13/50 [00:01<00:04,  8.32it/s]\u001b[A\n",
            " 28%|██▊       | 14/50 [00:01<00:04,  8.27it/s]\u001b[A\n",
            " 30%|███       | 15/50 [00:01<00:04,  8.25it/s]\u001b[A\n",
            " 32%|███▏      | 16/50 [00:01<00:04,  8.24it/s]\u001b[A\n",
            " 34%|███▍      | 17/50 [00:01<00:03,  8.26it/s]\u001b[A\n",
            " 36%|███▌      | 18/50 [00:02<00:03,  8.20it/s]\u001b[A\n",
            " 38%|███▊      | 19/50 [00:02<00:03,  8.20it/s]\u001b[A\n",
            " 40%|████      | 20/50 [00:02<00:03,  8.20it/s]\u001b[A\n",
            " 42%|████▏     | 21/50 [00:02<00:03,  8.22it/s]\u001b[A\n",
            " 44%|████▍     | 22/50 [00:02<00:03,  8.23it/s]\u001b[A\n",
            " 46%|████▌     | 23/50 [00:02<00:03,  8.19it/s]\u001b[A\n",
            " 48%|████▊     | 24/50 [00:02<00:03,  8.22it/s]\u001b[A\n",
            " 50%|█████     | 25/50 [00:02<00:03,  8.21it/s]\u001b[A\n",
            " 52%|█████▏    | 26/50 [00:03<00:02,  8.21it/s]\u001b[A\n",
            " 54%|█████▍    | 27/50 [00:03<00:02,  8.15it/s]\u001b[A\n",
            " 56%|█████▌    | 28/50 [00:03<00:02,  8.16it/s]\u001b[A\n",
            " 58%|█████▊    | 29/50 [00:03<00:02,  8.19it/s]\u001b[A\n",
            " 60%|██████    | 30/50 [00:03<00:02,  8.15it/s]\u001b[A\n",
            " 62%|██████▏   | 31/50 [00:03<00:02,  8.23it/s]\u001b[A\n",
            " 64%|██████▍   | 32/50 [00:03<00:02,  8.29it/s]\u001b[A\n",
            " 66%|██████▌   | 33/50 [00:03<00:02,  8.31it/s]\u001b[A\n",
            " 68%|██████▊   | 34/50 [00:04<00:01,  8.18it/s]\u001b[A\n",
            " 70%|███████   | 35/50 [00:04<00:01,  8.16it/s]\u001b[A\n",
            " 72%|███████▏  | 36/50 [00:04<00:01,  8.20it/s]\u001b[A\n",
            " 74%|███████▍  | 37/50 [00:04<00:01,  8.27it/s]\u001b[A\n",
            " 76%|███████▌  | 38/50 [00:04<00:01,  7.70it/s]\u001b[A\n",
            " 78%|███████▊  | 39/50 [00:04<00:01,  7.89it/s]\u001b[A\n",
            " 80%|████████  | 40/50 [00:04<00:01,  8.07it/s]\u001b[A\n",
            " 82%|████████▏ | 41/50 [00:05<00:03,  2.36it/s]\u001b[A\n",
            " 84%|████████▍ | 42/50 [00:07<00:05,  1.58it/s]\u001b[A\n",
            " 86%|████████▌ | 43/50 [00:07<00:03,  2.09it/s]\u001b[A\n",
            " 88%|████████▊ | 44/50 [00:07<00:02,  2.69it/s]\u001b[A\n",
            " 90%|█████████ | 45/50 [00:07<00:01,  3.37it/s]\u001b[A\n",
            " 92%|█████████▏| 46/50 [00:07<00:00,  4.11it/s]\u001b[A\n",
            " 94%|█████████▍| 47/50 [00:08<00:01,  1.97it/s]\u001b[A\n",
            " 96%|█████████▌| 48/50 [00:09<00:01,  1.45it/s]\u001b[A\n",
            " 98%|█████████▊| 49/50 [00:09<00:00,  1.89it/s]\u001b[A\n",
            "100%|██████████| 50/50 [00:10<00:00,  2.46it/s]\u001b[A\n",
            "51it [00:10,  3.12it/s]                        \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss -0.1445927619934082 :: EP reward -1\n",
            "Loss 0.11999654769897461 :: EP reward -1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IVwoOeX4Cuf"
      },
      "source": [
        "# Training a diad of SimpleOnPolicyRLAgent agents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3XqiJqKoAHK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "outputId": "7bcd5b13-48e3-4b1e-b963-a230aa5d4c11"
      },
      "source": [
        "use_cuda = True \n",
        "sparse_reward = False\n",
        "\n",
        "agent1 = SimpleOnPolicyRLAgent( \n",
        "  learning_rate=1e-4,\n",
        "  discount_factor=0.99,\n",
        "  num_actions=5,\n",
        "  pov_shape=[7,7,12],\n",
        "  use_cuda=use_cuda,\n",
        ")\n",
        "\n",
        "agent2 = SimpleOnPolicyRLAgent( \n",
        "  learning_rate=1e-4,\n",
        "  discount_factor=0.99,\n",
        "  num_actions=5,\n",
        "  pov_shape=[7,7,12],\n",
        "  use_cuda=use_cuda,\n",
        ")\n",
        "\n",
        "#logging_path = './test_training.log'\n",
        "#logger = SummaryWriter(logging_path)\n",
        "\n",
        "max_episode_length = 50\n",
        "nbr_training_episodes = 1000\n",
        "verbose = False \n",
        "\n",
        "tbar = tqdm(total=nbr_training_episodes, position=0)\n",
        "for episode in range(nbr_training_episodes):\n",
        "  tbar.update(1)\n",
        "  environment_kwargs = {\n",
        "      \"level\":\"1\",\n",
        "      \"sparse_reward\":sparse_reward,\n",
        "      \"verbose\":verbose,\n",
        "  }\n",
        "  environment = TwoPlayersCoMazeGym(**environment_kwargs)\n",
        "\n",
        "  episode_cum_reward, trajectory = two_players_environment_loop(\n",
        "      agent1=agent1,\n",
        "      agent2=agent2,\n",
        "      environment=environment,\n",
        "      max_episode_length=max_episode_length,\n",
        "  )\n",
        "\n",
        "  #logger.add_scalar(\"Training/EpisodeCumulativeReward\", episode_cum_reward, episode)\n",
        "  #logger.add_scalar(\"Training/NbrSteps\", len(trajectory), episode)\n",
        "  #logger.flush()\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4%|▍         | 2/50 [00:00<00:02, 16.94it/s]\u001b[A\n",
            "  6%|▌         | 3/50 [00:00<00:03, 13.05it/s]\u001b[A\n",
            "  8%|▊         | 4/50 [00:00<00:04, 11.16it/s]\u001b[A\n",
            " 10%|█         | 5/50 [00:00<00:04, 10.03it/s]\u001b[A\n",
            " 12%|█▏        | 6/50 [00:00<00:04,  9.34it/s]\u001b[A\n",
            " 14%|█▍        | 7/50 [00:00<00:04,  8.97it/s]\u001b[A\n",
            " 16%|█▌        | 8/50 [00:00<00:04,  8.70it/s]\u001b[A\n",
            " 18%|█▊        | 9/50 [00:00<00:04,  8.49it/s]\u001b[A\n",
            " 20%|██        | 10/50 [00:01<00:04,  8.46it/s]\u001b[A\n",
            " 22%|██▏       | 11/50 [00:01<00:04,  8.49it/s]\u001b[A\n",
            " 24%|██▍       | 12/50 [00:01<00:04,  8.40it/s]\u001b[A\n",
            " 26%|██▌       | 13/50 [00:01<00:04,  8.37it/s]\u001b[A\n",
            " 28%|██▊       | 14/50 [00:01<00:04,  8.35it/s]\u001b[A\n",
            " 30%|███       | 15/50 [00:01<00:04,  8.38it/s]\u001b[A\n",
            " 32%|███▏      | 16/50 [00:01<00:04,  8.36it/s]\u001b[A\n",
            " 34%|███▍      | 17/50 [00:01<00:03,  8.28it/s]\u001b[A\n",
            " 36%|███▌      | 18/50 [00:02<00:03,  8.32it/s]\u001b[A\n",
            " 38%|███▊      | 19/50 [00:02<00:03,  8.28it/s]\u001b[A\n",
            " 40%|████      | 20/50 [00:02<00:03,  8.32it/s]\u001b[A\n",
            " 42%|████▏     | 21/50 [00:02<00:03,  8.26it/s]\u001b[A\n",
            " 44%|████▍     | 22/50 [00:02<00:03,  8.24it/s]\u001b[A\n",
            " 46%|████▌     | 23/50 [00:02<00:03,  8.13it/s]\u001b[A\n",
            " 48%|████▊     | 24/50 [00:02<00:03,  8.14it/s]\u001b[A\n",
            " 50%|█████     | 25/50 [00:02<00:03,  8.19it/s]\u001b[A\n",
            " 52%|█████▏    | 26/50 [00:03<00:02,  8.24it/s]\u001b[A\n",
            " 54%|█████▍    | 27/50 [00:03<00:02,  8.20it/s]\u001b[A\n",
            " 56%|█████▌    | 28/50 [00:03<00:02,  8.22it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/content/comaze-python/comaze/env/comaze.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-82170c97b331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0magent2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0mmax_episode_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_episode_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   )\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-5fe8221ae768>\u001b[0m in \u001b[0;36mtwo_players_environment_loop\u001b[0;34m(agent1, agent2, environment, max_episode_length)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Progress simulation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mmax_episode_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/comaze-python/comaze/env/comaze.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"WARNING: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m     \u001b[0mresulting_game_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'---'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HBhpKTQr2SB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXKqVHxQUg2Q"
      },
      "source": [
        "# 2) Create a communicating On-Policy RL Agent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIRiZtT1Uxr6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}